6.885 Lecture 7 (Entity Resolution Techniques Continued and MapReduce) - 9/26/13

**MACHINE LEARNING EXAMPLE FROM LAST TIME**
- Training -> given tuples of (features, match), you generate a model
- Testing -> use the model to try and get a result

for e1 in dbl
  for e2 in db2
    s = similarity(e1, e2)
    if s > thres
      output match

Decision Trees
- First split the data on some point (A1 > x)
  - on the two resulting partitions, split again (left side A2 < y and right side A3 < z)
- if you make the depth of your decision tree too deep, you are overfitting the data
  - does great on training data (as depth increases, you approach the case where there's a bucket for each datapoint)
  - but it will fail on the testing data, because you overfitted

**MAP REDUCE**
- falls under the storage and processing part of the data pipeline
- mappers write data to local disks, and the reducers pull data from the local disks to process
  - keys are all mapped to a certain reducer (so for a given key, they all go to the same reducer.)

Example
- Want to run 'SELECT e_name, e_age, d_name FROM emp, dept WHERE e.did = dept.did'
- How would you do this in MapReduce?
  - You can have the mappers emit the 'did' as the key, and have the relevant attributes as the value

  - Sam Madden's solution:
    - Map(Key tableName, Value record):
        out.tab = tableName
        out.rec = record
          emit (record["dno"], out)

      Reduce(Key dno, Values recs[]):
        for r1 in recs:
          for r2 in recs:
            if r1.tableName != r2.tableName:
              out = join(r1, r2)
              emit (dno, out)

**PAPERS - MAPREDUCE: A FLEXIBLE DATA PROCESSING TOOL**
**PAPERS - POSSIBLE HADOOP TRAJECTORIES**

MapReduce Criticisms
- bad for iterative algorithms (e.g. graphs)
  - slow for MapReduce, because this involves keeping state, and for MapReduce this means an expensive disk write
  - MapReduce writes intermediate state to disk = BAD
- all operations are scans (no indexes) -> WRONG
  - MapReduce DOES support indices, but you have to do extra work to do it
    - you could have a special input reader with an indexing facility to skip files you don't need
    - you could also have the date of log files in the name, so you can "index" it that way
    - many ways to do things that are "akin" to indexing
- long startup times -> WRONG
- you have to do query optimization yourself (hard to get good performance without a lot of tuning)
- MapReduce does not have point to point communication

MapReduce Benefits
- MapReduce has been shown to be good at Google for:
  - page rank(?)
  - image stitching
  - inverted index constructions
  - path precomputation in maps
- Especially pagerank and inverted index contruction, MapReduce is good because you don't need to load the data into a database system.
  - You only need to do it once, so no need to load

Hive - a relational database system built on top of MapReduce
